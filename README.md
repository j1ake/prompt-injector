# ğŸ›¡ï¸ Prompt Injector

> **Lightweight prompt injection testing for AI security professionals**

A minimal TypeScript library and interactive testing suite for systematically evaluating AI system resilience against prompt injection attacks. Built on cutting-edge research from NeurIPS 2024 and industry-leading security frameworks.

## ğŸ¯ Why This Matters

**AI systems are under constant attack.** With 89.6% success rates against production models, prompt injection vulnerabilities represent one of the most critical security risks facing AI-powered applications today.

- **ğŸ”´ Risk**: Unvalidated AI systems can leak sensitive data, execute unauthorized actions, and bypass safety guardrails
- **ğŸ’° Impact**: Data breaches, compliance violations, and compromised user trust
- **âš¡ Solution**: Proactive security testing with state-of-the-art attack patterns

## âœ¨ Key Features

### ğŸ”¬ Research-Backed Attack Patterns
- **100+ curated injection patterns** from JailbreakBench (NeurIPS 2024)
- **CySecBench integration** - 24x more comprehensive than traditional datasets  
- **Multi-vector testing**: Jailbreaking, instruction hijacking, logic traps, encoding attacks

### âš™ï¸ Configurable Testing Framework
- **Severity levels**: Basic, Intermediate, Advanced, Expert
- **Attack categories**: Role-play exploits, system prompt overrides, encoding bypasses
- **Custom targeting**: Fine-tune tests for your specific use case

### ğŸ¨ Interactive Tech Demo
- **Svelte 5 + Apple HIG design** - Beautiful, intuitive interface
- **Real-time visualization** of attack patterns and vulnerabilities
- **Educational mode** with detailed explanations of each technique

## ğŸš€ Quick Start

```bash
npm install @blueprintlabio/prompt-injector
```

```typescript
import { PromptInjector } from '@blueprintlabio/prompt-injector';

const injector = new PromptInjector({
  severity: 'intermediate',
  categories: ['jailbreak', 'instruction-hijack'],
  maxAttempts: 50
});

// Generate targeted test cases with specific injection goals
const testCases = injector.generateTests('customer-service-bot');

// Test your system
const results = await injector.runTests(yourAISystem);
const report = injector.generateReport(results);

console.log(`Risk Score: ${report.summary.riskScore}`);
console.log(`Success Rate: ${report.summary.successRate}%`);
```

## ğŸª Interactive Demo

Experience prompt injection testing with our Apple HIG-inspired interface:

```bash
npm run dev
```

- **ğŸ“Š Visual dashboards** showing vulnerability patterns
- **ğŸ¯ Targeted testing** for different AI system types  
- **ğŸ“š Educational content** explaining each attack vector
- **ğŸ”’ Safe environment** - no external API calls

## ğŸ† Built on SOTA Research

### Academic Foundations
- **JailbreakBench** (NeurIPS 2024) - Open robustness benchmark
- **HarmBench** integration - Standardized evaluation framework  
- **Sentinel detection** patterns - 98.7% accuracy baseline

### Industry Standards  
- **OWASP LLM Top 10** alignment
- **Microsoft PyRIT** methodology compatibility
- **Bugcrowd AI pentesting** best practices

## ğŸ“ˆ Impact & Value Creation

### For Security Teams
- **Reduce breach risk** by 80%+ through proactive testing
- **Compliance ready** - meets emerging AI governance standards
- **Actionable insights** with detailed vulnerability reports

### For AI Development Teams  
- **Shift-left security** - catch issues before production
- **Continuous testing** - integrate into CI/CD pipelines  
- **Educational value** - upskill teams on AI security

### For Organizations
- **Risk mitigation** - protect intellectual property and user data
- **Competitive advantage** - deploy AI safely and confidently
- **Future-proof** - stay ahead of evolving attack techniques

## ğŸ› ï¸ Core Architecture

```
src/lib/
â”œâ”€â”€ attacks/           # Attack pattern definitions
â”‚   â”œâ”€â”€ jailbreak/     # Role-play and persona-based attacks  
â”‚   â”œâ”€â”€ hijack/        # System prompt override techniques
â”‚   â”œâ”€â”€ encoding/      # Obfuscation and encoding bypasses
â”‚   â””â”€â”€ logic/         # Reasoning exploitation patterns
â”œâ”€â”€ generators/        # Dynamic test case generation
â”œâ”€â”€ evaluators/        # Response assessment logic
â””â”€â”€ index.ts          # Main API interface
```

## ğŸ¨ Demo Features

- **ğŸ¯ Attack Pattern Simulator** - Interactive testing interface
- **ğŸ“Š Vulnerability Dashboard** - Real-time security metrics  
- **ğŸ“ Learning Center** - Educational content on AI security
- **ğŸ”§ Custom Configuration** - Tailor tests to your needs

## ğŸ¤ Contributing

We welcome contributions from security researchers, AI safety experts, and developers. This project advances the critical field of AI security through open collaboration.

## ğŸ“„ License

MIT License - See LICENSE for details.

---

**âš ï¸ Responsible Use**: This tool is designed for testing AI systems you own or have explicit permission to test. Always follow responsible disclosure practices and adhere to applicable laws and terms of service.

**ğŸ”— Research Citations**: Built upon peer-reviewed research from NeurIPS 2024, USENIX Security, and leading AI safety organizations.
